{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6960eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3f041-d6ff-4bfd-92f6-042cb217a562",
   "metadata": {},
   "source": [
    "This demo covers the basics of MLLib with an example Regression problem on the California housing dataset [found here](https://www.kaggle.com/camnugent/california-housing-prices).\n",
    "\n",
    "References in the PySpark MLLib Library can be found at:\n",
    "[pyspark mllib api](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df38a8-07e5-43cd-bfbb-37282d9dffe3",
   "metadata": {},
   "source": [
    "Note for below: Spark integrates with libsvm format seamlessly which is a text format in which each line represents a labeled sparse feature vector using the following format: label index1:value1 index2:value2 via spark.read.format(\"libsvm\").load(\"data/mllib/sample_linear_regression_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25644f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = pd.concat([housing['data'], housing['target']], axis=1)\n",
    "\n",
    "# other formats\n",
    "# df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "# df = spark.read.load(\"examples/src/main/resources/people.csv\", format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720a3aa-2316-4874-9fd4-0d9a20f53008",
   "metadata": {},
   "source": [
    "\n",
    "SparkSession is the entrypoint for all Python based Spark programs, provides the functionality and access to RDD's. It is very similar to opening the Spark CMD line Interface and utilizing the SparkSession object automatically provided to you.\n",
    "\n",
    "Parameters:<br> \n",
    "master => Refers to the main/master resource in the cluster depending on if your using yarn or mesos for resource management. We use local\\[x\\] below for standalone mode, the * represents number of partitions to use when creating an RDD/DataFrame/Dataset - better utilizes with # of cores u have. <br>\n",
    "appname => name of app <br>\n",
    "configs => any configs necessary (we add port/driver address to utilize the spark built in UI to see jobs) <br>\n",
    "getOrCreate => creates app if not already active (make sure to spark.stop() at end of session <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc6e8bb-6e54-404d-9168-72f0fe8dd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/26 17:15:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ML Lib Example\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"localhost\") \\\n",
    "    .config(\"spark.ui.port\",\"8080\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c94eb90-c3fb-47d1-822c-51b83a397912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MedInc: double (nullable = true)\n",
      " |-- HouseAge: double (nullable = true)\n",
      " |-- AveRooms: double (nullable = true)\n",
      " |-- AveBedrms: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- AveOccup: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- MedHouseVal: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----------+\n",
      "|MedInc|HouseAge|          AveRooms|         AveBedrms|Population|          AveOccup|Latitude|Longitude|MedHouseVal|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----------+\n",
      "|8.3252|    41.0| 6.984126984126984|1.0238095238095237|     322.0|2.5555555555555554|   37.88|  -122.23|      4.526|\n",
      "|8.3014|    21.0| 6.238137082601054|0.9718804920913884|    2401.0| 2.109841827768014|   37.86|  -122.22|      3.585|\n",
      "|7.2574|    52.0| 8.288135593220339| 1.073446327683616|     496.0|2.8022598870056497|   37.85|  -122.24|      3.521|\n",
      "|5.6431|    52.0|5.8173515981735155|1.0730593607305936|     558.0| 2.547945205479452|   37.85|  -122.25|      3.413|\n",
      "|3.8462|    52.0| 6.281853281853282|1.0810810810810811|     565.0|2.1814671814671813|   37.85|  -122.25|      3.422|\n",
      "|4.0368|    52.0| 4.761658031088083|1.1036269430051813|     413.0| 2.139896373056995|   37.85|  -122.25|      2.697|\n",
      "|3.6591|    52.0|4.9319066147859925|0.9513618677042801|    1094.0|2.1284046692607004|   37.84|  -122.25|      2.992|\n",
      "|  3.12|    52.0| 4.797527047913447| 1.061823802163833|    1157.0|1.7882534775888717|   37.84|  -122.25|      2.414|\n",
      "|2.0804|    42.0| 4.294117647058823|1.1176470588235294|    1206.0| 2.026890756302521|   37.84|  -122.26|      2.267|\n",
      "|3.6912|    52.0| 4.970588235294118|0.9901960784313726|    1551.0| 2.172268907563025|   37.84|  -122.25|      2.611|\n",
      "|3.2031|    52.0| 5.477611940298507|1.0796019900497513|     910.0| 2.263681592039801|   37.85|  -122.26|      2.815|\n",
      "|3.2705|    52.0| 4.772479564032698|1.0245231607629428|    1504.0|2.0490463215258856|   37.85|  -122.26|      2.418|\n",
      "| 3.075|    52.0| 5.322649572649572|1.0128205128205128|    1098.0|2.3461538461538463|   37.85|  -122.26|      2.135|\n",
      "|2.6736|    52.0|               4.0|1.0977011494252873|     345.0|1.9827586206896552|   37.84|  -122.26|      1.913|\n",
      "|1.9167|    52.0| 4.262903225806451|1.0096774193548388|    1212.0|1.9548387096774194|   37.85|  -122.26|      1.592|\n",
      "| 2.125|    50.0| 4.242424242424242| 1.071969696969697|     697.0| 2.640151515151515|   37.85|  -122.26|        1.4|\n",
      "| 2.775|    52.0|5.9395770392749245|1.0483383685800605|     793.0| 2.395770392749245|   37.85|  -122.27|      1.525|\n",
      "|2.1202|    52.0| 4.052805280528053| 0.966996699669967|     648.0|2.1386138613861387|   37.85|  -122.27|      1.555|\n",
      "|1.9911|    50.0| 5.343675417661098|1.0859188544152745|     990.0|2.3627684964200477|   37.84|  -122.26|      1.587|\n",
      "|2.6033|    52.0| 5.465454545454546|1.0836363636363637|     690.0|2.5090909090909093|   37.84|  -122.27|      1.629|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create PySpark DataFrame from Pandas\n",
    "\n",
    "df = spark.createDataFrame(df) # can also use toDF() function\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08b72b-1663-4ef5-a33d-01929769b863",
   "metadata": {},
   "source": [
    "Dense Vectors represent value arrays similar to the underlying type - numpy arrays (.toArray() gives you underlying numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ccff6d-0933-4a78-b4eb-887e5d49f2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|4.526|[8.3252,41.0,6.98...|\n",
      "|3.585|[8.3014,21.0,6.23...|\n",
      "|3.521|[7.2574,52.0,8.28...|\n",
      "|3.413|[5.6431,52.0,5.81...|\n",
      "|3.422|[3.8462,52.0,6.28...|\n",
      "|2.697|[4.0368,52.0,4.76...|\n",
      "|2.992|[3.6591,52.0,4.93...|\n",
      "|2.414|[3.12,52.0,4.7975...|\n",
      "|2.267|[2.0804,42.0,4.29...|\n",
      "|2.611|[3.6912,52.0,4.97...|\n",
      "|2.815|[3.2031,52.0,5.47...|\n",
      "|2.418|[3.2705,52.0,4.77...|\n",
      "|2.135|[3.075,52.0,5.322...|\n",
      "|1.913|[2.6736,52.0,4.0,...|\n",
      "|1.592|[1.9167,52.0,4.26...|\n",
      "|  1.4|[2.125,50.0,4.242...|\n",
      "|1.525|[2.775,52.0,5.939...|\n",
      "|1.555|[2.1202,52.0,4.05...|\n",
      "|1.587|[1.9911,50.0,5.34...|\n",
      "|1.629|[2.6033,52.0,5.46...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# x[:1-1] are the features and x[-1] is the target variable\n",
    "data = df.rdd.map(lambda x: (x[-1], DenseVector(x[:-1]))) \n",
    "\n",
    "# creating a dataframe with columns ‘label’ (target variable) and #‘features’(dense vector of independent variables)\n",
    "df2 = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19a4fd-a4db-418d-8403-79335f933169",
   "metadata": {},
   "source": [
    "Scalers :<br> \n",
    "pyspark.ml.feature.MaxAbsScaler (Python class, in MaxAbsScaler)<br>\n",
    "pyspark.ml.feature.MaxAbsScalerModel (Python class, in MaxAbsScalerModel)<br>\n",
    "pyspark.ml.feature.MinMaxScaler (Python class, in MinMaxScaler)<br>\n",
    "pyspark.ml.feature.MinMaxScalerModel (Python class, in MinMaxScalerModel)<br>\n",
    "pyspark.ml.feature.RobustScaler (Python class, in RobustScaler)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a3bc4b-bd47-4f83-bcb5-08bfb69c4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|     scaled_features|\n",
      "+-----+--------------------+--------------------+\n",
      "|4.526|[8.3252,41.0,6.98...|[4.38209539419521...|\n",
      "|3.585|[8.3014,21.0,6.23...|[4.36956790291790...|\n",
      "|3.521|[7.2574,52.0,8.28...|[3.82004265529144...|\n",
      "|3.413|[5.6431,52.0,5.81...|[2.97033134567133...|\n",
      "|3.422|[3.8462,52.0,6.28...|[2.02450575423456...|\n",
      "|2.697|[4.0368,52.0,4.76...|[2.12483095748897...|\n",
      "|2.992|[3.6591,52.0,4.93...|[1.92602282910917...|\n",
      "|2.414|[3.12,52.0,4.7975...|[1.64225936072275...|\n",
      "|2.267|[2.0804,42.0,4.29...|[1.09505011988705...|\n",
      "|2.611|[3.6912,52.0,4.97...|[1.94291915137814...|\n",
      "|2.815|[3.2031,52.0,5.47...|[1.68600030715738...|\n",
      "|2.418|[3.2705,52.0,4.77...|[1.72147732027043...|\n",
      "|2.135|[3.075,52.0,5.322...|[1.61857292763540...|\n",
      "|1.913|[2.6736,52.0,4.0,...|[1.40728994449626...|\n",
      "|1.592|[1.9167,52.0,4.26...|[1.00888413996708...|\n",
      "|  1.4|[2.125,50.0,4.242...|[1.11852600690251...|\n",
      "|1.525|[2.775,52.0,5.939...|[1.46066337371975...|\n",
      "|1.555|[2.1202,52.0,4.05...|[1.11599945403986...|\n",
      "|1.587|[1.9911,50.0,5.34...|[1.04804570933816...|\n",
      "|1.629|[2.6033,52.0,5.46...|[1.37028647236203...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "ss = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler = ss.fit(df2)\n",
    "scaled_df = scaler.transform(df2)\n",
    "scaled_df.printSchema()\n",
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af9598d-fbb9-49cd-a959-c12b1a0fa1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Train Shape: (16432, 3), Test Shape: (4208, 3)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data\n",
    "train, test = scaled_df.randomSplit([.8,.2],seed=1)\n",
    "(\"Train Shape: {}, Test Shape: {}\").format((train.count(), len(train.columns)), (test.count(), len(test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f2aab3-f61f-412c-a49b-3ed9d2d955a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "linearReg = LinearRegression(featuresCol = \"scaled_features\", labelCol = \"label\") # fit the model to the the training data\n",
    "model = linearReg.fit(train) # make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed2c0ce-6bb4-46ea-aee1-3bc53d38e87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------------------+\n",
      "|label|            features|     scaled_features|        prediction|\n",
      "+-----+--------------------+--------------------+------------------+\n",
      "| 0.72|[2.57,52.0,4.2021...|[1.35275851187739...| 2.129807202999551|\n",
      "|0.764|[2.5192,43.0,4.88...|[1.32601916074767...|1.8919277100759402|\n",
      "|0.784|[1.1667,52.0,5.40...|[0.61411025517796...|1.4160110704872366|\n",
      "|0.797|[2.1111,44.0,4.02...|[1.11120953090442...|1.7189627294627527|\n",
      "|  0.8|[2.9063,42.0,4.59...|[1.52977512181683...|  2.11349438758441|\n",
      "|0.824|[2.455,42.0,5.223...|[1.29222651620973...| 1.863731439139336|\n",
      "|0.831|[1.4113,52.0,4.29...|[0.74285917813718...| 1.612754622773167|\n",
      "|0.833|[2.3869,16.0,4.79...|[1.25638104747087...|1.5877001472916135|\n",
      "|0.851|[1.6196,38.0,3.83...|[0.85250104507261...|1.5812261158514005|\n",
      "|0.857|[1.75,47.0,4.0,0....|[0.92113906450795...| 1.621465991567753|\n",
      "|0.875|[1.6098,52.0,5.02...|[0.84734266631137...|1.5651671736329646|\n",
      "|0.889|[2.2467,46.0,5.94...|[1.18258464927429...| 1.733452915251867|\n",
      "|  0.9|[1.4375,48.0,5.01...|[0.75664994584581...|1.5017644031919275|\n",
      "|0.918|[2.4962,37.0,5.32...|[1.31391276161414...|1.8169976783038635|\n",
      "|0.948|[1.8426,40.0,4.92...|[0.96988048014991...|1.5359636951294462|\n",
      "|0.948|[2.2562,39.0,5.00...|[1.18758511848162...|1.7632251691855387|\n",
      "|0.967|[1.8407,49.0,5.11...|[0.96888038630845...| 1.601495639252981|\n",
      "|0.972|[1.4861,49.0,4.60...|[0.78223129358015...|1.5573855671266017|\n",
      "|0.975|[1.7772,45.0,3.88...|[0.93545619739630...|1.7615871828665917|\n",
      "|0.982|[2.4083,52.0,6.72...|[1.26764526231686...|1.8878131349642686|\n",
      "+-----+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictions = model.transform(test) # show the predicted values and the actual values\n",
    "predictions.select(\"prediction\" , \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72aba2c-8bb7-4c39-aad3-b5bb1a6da2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on test set:  0.6122841723689958\n"
     ]
    }
   ],
   "source": [
    "# Evaluations\n",
    "r2_test = model.evaluate(test).r2\n",
    "print(\"R2 score on test set: \", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9537e95e-f151-453c-b75d-3796cee333b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 0.7132556397149299,\n",
      "MAE on test set: 0.5259975078213824, \n",
      "MSE on test set: 0.5087336075851538\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator # RMSE on test set\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "\n",
    "rmse_test = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "mae_test = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "mse_test = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "print(\"RMSE on test set: {},\\nMAE on test set: {}, \\nMSE on test set: {}\".format(rmse_test, mae_test, mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3fb59-58ba-4148-95c2-b0e6375431f3",
   "metadata": {},
   "source": [
    "## Classification & Pipeline Example\n",
    "\n",
    "Uses the iris dataset for classifying iris plants given petal length, petal width, sepal length, sepal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16dfc008-f637-4394-9690-af5fbcec9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "\n",
    "iris_schema = StructType([\n",
    "    StructField(\"sepal_length\", DoubleType(), False),\n",
    "    StructField(\"sepal_width\", DoubleType(), False),\n",
    "    StructField(\"petal_length\", DoubleType(), False),\n",
    "    StructField(\"petal_width\", DoubleType(), False),\n",
    "    StructField(\"class\", StringType(), False)\n",
    "])\n",
    "\n",
    "iris_df = spark.read.schema(iris_schema).load(\"iris.csv\", format=\"csv\")\n",
    "iris_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b56b63d-27a5-490b-be7c-eb3325d8434f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = iris_df.rdd.map(lambda x: (x[-1], DenseVector(x[:-1]))) \n",
    "# creating a dataframe with columns ‘label’ (target variable) and #‘features’(dense vector of independent variables)\n",
    "df2 = spark.createDataFrame(data, schema=['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fbcda55-95d4-4df1-9216-2e8fa72cc27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|      label|         features|\n",
      "+-----------+-----------------+\n",
      "|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "|Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
      "|Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
      "|Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
      "|Iris-setosa|[5.4,3.9,1.7,0.4]|\n",
      "|Iris-setosa|[4.6,3.4,1.4,0.3]|\n",
      "|Iris-setosa|[5.0,3.4,1.5,0.2]|\n",
      "|Iris-setosa|[4.4,2.9,1.4,0.2]|\n",
      "|Iris-setosa|[4.9,3.1,1.5,0.1]|\n",
      "|Iris-setosa|[5.4,3.7,1.5,0.2]|\n",
      "|Iris-setosa|[4.8,3.4,1.6,0.2]|\n",
      "|Iris-setosa|[4.8,3.0,1.4,0.1]|\n",
      "|Iris-setosa|[4.3,3.0,1.1,0.1]|\n",
      "|Iris-setosa|[5.8,4.0,1.2,0.2]|\n",
      "|Iris-setosa|[5.7,4.4,1.5,0.4]|\n",
      "|Iris-setosa|[5.4,3.9,1.3,0.4]|\n",
      "|Iris-setosa|[5.1,3.5,1.4,0.3]|\n",
      "|Iris-setosa|[5.7,3.8,1.7,0.3]|\n",
      "|Iris-setosa|[5.1,3.8,1.5,0.3]|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95449bf5-b765-47f2-8f36-33b12eb82f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = StringIndexer(inputCol='label', outputCol='label_class')\n",
    "scaling = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "multi_lr = LogisticRegression(maxIter=2, regParam=0.3, elasticNetParam=0.8, featuresCol = \"scaled_features\", labelCol = \"label_class\")\n",
    "\n",
    "pipeline = Pipeline(stages=[label_encoder, scaling, multi_lr]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d71346b-3a76-4620-8c2d-b112a7a42db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------+--------------------+--------------------+--------------------+----------+\n",
      "|      label|         features|label_class|     scaled_features|       rawPrediction|         probability|prediction|\n",
      "+-----------+-----------------+-----------+--------------------+--------------------+--------------------+----------+\n",
      "|Iris-setosa|[5.1,3.5,1.4,0.2]|        0.0|[6.15892840883878...|[0.50415620379330...|[0.55187226587420...|       0.0|\n",
      "|Iris-setosa|[4.9,3.0,1.4,0.2]|        0.0|[5.9174018045706,...|[0.49309454006432...|[0.54960500935144...|       0.0|\n",
      "|Iris-setosa|[4.7,3.2,1.3,0.2]|        0.0|[5.67587520030241...|[0.53860721388051...|[0.56183407326107...|       0.0|\n",
      "|Iris-setosa|[4.6,3.1,1.5,0.2]|        0.0|[5.55511189816831...|[0.51191378171187...|[0.55443358078581...|       0.0|\n",
      "|Iris-setosa|[5.0,3.6,1.4,0.2]|        0.0|[6.03816510670469...|[0.51939642107407...|[0.55587274798294...|       0.0|\n",
      "|Iris-setosa|[5.4,3.9,1.7,0.4]|        0.0|[6.52121831524107...|[0.39389084523775...|[0.51823010110859...|       0.0|\n",
      "|Iris-setosa|[4.6,3.4,1.4,0.3]|        0.0|[5.55511189816831...|[0.51425473375680...|[0.55364783912124...|       0.0|\n",
      "|Iris-setosa|[5.0,3.4,1.5,0.2]|        0.0|[6.03816510670469...|[0.49249501087929...|[0.54869123421516...|       0.0|\n",
      "|Iris-setosa|[4.4,2.9,1.4,0.2]|        0.0|[5.31358529390013...|[0.53368811364775...|[0.56079580460639...|       0.0|\n",
      "|Iris-setosa|[4.9,3.1,1.5,0.1]|        0.0|[5.9174018045706,...|[0.51449192989969...|[0.55625009325017...|       0.0|\n",
      "|Iris-setosa|[5.4,3.7,1.5,0.2]|        0.0|[6.52121831524107...|[0.47307624004671...|[0.54293095711984...|       0.0|\n",
      "|Iris-setosa|[4.8,3.4,1.6,0.2]|        0.0|[5.79663850243650...|[0.49607403524606...|[0.54951742869656...|       0.0|\n",
      "|Iris-setosa|[4.8,3.0,1.4,0.1]|        0.0|[5.79663850243650...|[0.53289521549495...|[0.56154009947677...|       0.0|\n",
      "|Iris-setosa|[4.3,3.0,1.1,0.1]|        0.0|[5.19282199176603...|[0.62452009231235...|[0.58661077259775...|       0.0|\n",
      "|Iris-setosa|[5.8,4.0,1.2,0.2]|        0.0|[7.00427152377744...|[0.49875418697803...|[0.54994898233813...|       0.0|\n",
      "|Iris-setosa|[5.7,4.4,1.5,0.4]|        0.0|[6.88350822164335...|[0.42571135566528...|[0.52653857686427...|       0.0|\n",
      "|Iris-setosa|[5.4,3.9,1.3,0.4]|        0.0|[6.52121831524107...|[0.45401980225627...|[0.53541347468326...|       0.0|\n",
      "|Iris-setosa|[5.1,3.5,1.4,0.3]|        0.0|[6.15892840883878...|[0.47366116017337...|[0.54240800915195...|       0.0|\n",
      "|Iris-setosa|[5.7,3.8,1.7,0.3]|        0.0|[6.88350822164335...|[0.39053440795550...|[0.51863010075277...|       0.0|\n",
      "|Iris-setosa|[5.1,3.8,1.5,0.3]|        0.0|[6.15892840883878...|[0.47643267732896...|[0.54255776488444...|       0.0|\n",
      "+-----------+-----------------+-----------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df2) \n",
    "prediction = model.transform(df2)\n",
    "\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17481a5b-ab40-4742-a7d4-9260570e1e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf74ab6-efdd-4c1a-9ffc-145a50dfee30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
